{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm\nimport cv2\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import densenet121, DenseNet121_Weights\nfrom torchvision import transforms\nimport torchvision\nfrom PIL import Image\n\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# COCO class names\nCOCO_CLASSES = [\n    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\nNUM_CLASSES = len(COCO_CLASSES)\n\n# Constants\nIMG_SIZE = 416  # YOLO standard size\nGRID_SIZES = [13, 26]  # Different grid sizes for multi-scale detection\n\n# Anchors (these should be optimized for your dataset using k-means clustering)\n# Format: width, height (normalized by image size)\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],  # Anchors for scale 13x13\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)]   # Anchors for scale 26x26\n]\n\n\n# DenseNet Backbone with YOLO Detection Heads\n# DenseNet Backbone with YOLO Detection Heads\nclass DenseNetYOLO(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(DenseNetYOLO, self).__init__()\n        \n        # Load DenseNet121 backbone\n        # Note: Don't use the backbone directly to avoid feature extraction issues\n        densenet = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n        \n        # DenseNet121 feature extractor layers - manually defined\n        # This avoids the complex feature extraction logic and ensures we have\n        # proper control over the layer outputs and feature maps\n        \n        # Initial convolution and pooling\n        self.conv0 = densenet.features.conv0\n        self.norm0 = densenet.features.norm0\n        self.relu0 = densenet.features.relu0\n        self.pool0 = densenet.features.pool0\n        \n        # Dense blocks and transition layers\n        self.denseblock1 = densenet.features.denseblock1\n        self.transition1 = densenet.features.transition1\n        \n        self.denseblock2 = densenet.features.denseblock2\n        self.transition2 = densenet.features.transition2\n        \n        self.denseblock3 = densenet.features.denseblock3\n        self.transition3 = densenet.features.transition3\n        \n        self.denseblock4 = densenet.features.denseblock4\n        self.norm5 = densenet.features.norm5\n        \n        # Freeze early layers\n        freeze_layers = [\n            self.conv0, self.norm0, self.relu0, self.pool0,\n            self.denseblock1, self.transition1,\n            self.denseblock2, self.transition2\n        ]\n        \n        for layer in freeze_layers:\n            for param in layer.parameters():\n                param.requires_grad = False\n        \n        # Number of output channels at each feature level\n        self.channels = {\n            'transition1': 128,  # After first transition\n            'transition2': 256,  # After second transition\n            'final': 1024        # After final dense block\n        }\n        \n        # Detection head for scale 1 (13x13)\n        self.detect1_conv = nn.Sequential(\n            nn.Conv2d(self.channels['final'], 512, kernel_size=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(1024, 512, kernel_size=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1)\n        )\n        self.detect1_output = nn.Conv2d(512, len(ANCHORS[0]) * (5 + num_classes), kernel_size=1)\n        \n        # Upsample for scale 2\n        self.upsample = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.1),\n            nn.Upsample(scale_factor=2, mode='nearest')\n        )\n        \n        # Detection head for scale 2 (26x26)\n        self.detect2_conv = nn.Sequential(\n            nn.Conv2d(256 + self.channels['transition2'], 256, kernel_size=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(512, 256, kernel_size=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.1)\n        )\n        self.detect2_output = nn.Conv2d(256, len(ANCHORS[1]) * (5 + num_classes), kernel_size=1)\n    \n    def forward(self, x):\n        # Store intermediate outputs for skip connections\n        feature_maps = {}\n        \n        # Initial layers\n        x = self.conv0(x)\n        x = self.norm0(x)\n        x = self.relu0(x)\n        x = self.pool0(x)\n        \n        # Dense block 1 and transition 1\n        x = self.denseblock1(x)\n        x = self.transition1(x)\n        feature_maps['transition1'] = x  # Save for possible future use\n        \n        # Dense block 2 and transition 2\n        x = self.denseblock2(x)\n        x = self.transition2(x)\n        feature_maps['transition2'] = x  # Save for skip connection\n        \n        # Dense block 3 and transition 3\n        x = self.denseblock3(x)\n        x = self.transition3(x)\n        \n        # Dense block 4 and final norm\n        x = self.denseblock4(x)\n        x = self.norm5(x)\n        feature_maps['final'] = x\n        \n        # Detection at scale 1 (13x13)\n        detect1_features = self.detect1_conv(x)\n        output1 = self.detect1_output(detect1_features)\n        \n        # Upsample for scale 2 (26x26)\n        upsampled = self.upsample(detect1_features)\n        \n        # Ensure the feature maps have compatible dimensions for concatenation\n        transition2 = feature_maps['transition2']\n        if upsampled.shape[2:] != transition2.shape[2:]:\n            # Resize if dimensions don't match\n            transition2 = F.interpolate(transition2, size=upsampled.shape[2:], mode='nearest')\n        \n        # Concatenate upsampled features with transition2 features\n        concat_features = torch.cat([upsampled, transition2], dim=1)\n        \n        # Detection at scale 2\n        detect2_features = self.detect2_conv(concat_features)\n        output2 = self.detect2_output(detect2_features)\n        \n        return [output1, output2]\n\n\n# Custom dataset class for COCO\nclass COCODataset(Dataset):\n    def __init__(self, img_dir, annotations_file, transform=None):\n        \"\"\"\n        A simple COCO dataset implementation.\n        \n        Args:\n            img_dir: Directory containing images\n            annotations_file: Path to COCO annotations JSON file\n            transform: Image transformations\n        \"\"\"\n        self.img_dir = img_dir\n        self.transform = transform\n        \n        # In a real implementation, you would use pycocotools to load annotations\n        # For simplicity, we're assuming a simple structure here\n        # You should replace this with proper COCO annotation loading\n        import json\n        with open(annotations_file, 'r') as f:\n            self.annotations = json.load(f)\n        \n        # Get image IDs\n        self.img_ids = [img['id'] for img in self.annotations['images']]\n        \n        # Create image ID to annotations mapping\n        self.id_to_annotations = {}\n        for ann in self.annotations['annotations']:\n            img_id = ann['image_id']\n            if img_id not in self.id_to_annotations:\n                self.id_to_annotations[img_id] = []\n            self.id_to_annotations[img_id].append(ann)\n            \n        # Create image ID to filename mapping\n        self.id_to_filename = {img['id']: img['file_name'] for img in self.annotations['images']}\n        \n        # Category ID to class index mapping\n        self.cat_id_to_class = {cat['id']: i for i, cat in enumerate(self.annotations['categories'])}\n    \n    def __len__(self):\n        return len(self.img_ids)\n    \n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_filename = self.id_to_filename[img_id]\n        img_path = os.path.join(self.img_dir, img_filename)\n        \n        # Load image\n        image = Image.open(img_path).convert('RGB')\n        orig_width, orig_height = image.size\n        \n        # Get annotations for this image\n        anns = self.id_to_annotations.get(img_id, [])\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        \n        # Convert bounding boxes to YOLO format:\n        # [class_idx, x_center, y_center, width, height] - normalized coordinates\n        targets = []\n        for ann in anns:\n            bbox = ann['bbox']  # COCO format: [x_min, y_min, width, height]\n            \n            # Convert to YOLO format\n            # Normalize coordinates\n            x_center = (bbox[0] + bbox[2] / 2) / orig_width\n            y_center = (bbox[1] + bbox[3] / 2) / orig_height\n            width = bbox[2] / orig_width\n            height = bbox[3] / orig_height\n            \n            # Get class index\n            class_idx = self.cat_id_to_class[ann['category_id']]\n            \n            # Only add if box is valid\n            if width > 0 and height > 0:\n                targets.append([class_idx, x_center, y_center, width, height])\n        \n        # Convert to tensor\n        targets = torch.tensor(targets, dtype=torch.float32)\n        \n        return image, targets\n\n\n# Function to convert model outputs to bounding boxes\ndef process_predictions(outputs, anchors, img_size=IMG_SIZE, conf_thresh=0.25):\n    \"\"\"\n    Process YOLO outputs to get bounding boxes and class predictions.\n    \n    Args:\n        outputs: List of tensors, one for each scale\n        anchors: List of anchors for each scale\n        img_size: Input image size\n        conf_thresh: Confidence threshold\n    \n    Returns:\n        List of detected boxes (batch_size, num_boxes, 6) where each box contains\n        [x1, y1, x2, y2, conf, class_idx]\n    \"\"\"\n    all_boxes = []\n    \n    for i, output in enumerate(outputs):\n        # Get grid size from output shape\n        batch_size, num_anchors_and_attrs, grid_size, _ = output.shape\n        \n        # Number of attributes per anchor: 5 (x, y, w, h, obj) + num_classes\n        num_attrs = (output.shape[1] // len(anchors[i]))\n        num_classes = num_attrs - 5\n        \n        # Reshape output\n        prediction = output.view(batch_size, len(anchors[i]), num_attrs, grid_size, grid_size)\n        prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()\n        \n        # Sigmoid object confidence and class scores\n        prediction[..., 4:] = torch.sigmoid(prediction[..., 4:])\n        \n        # Process each image in the batch\n        for b in range(batch_size):\n            # Only select boxes above confidence threshold\n            obj_mask = prediction[b, ..., 4] > conf_thresh\n            if not obj_mask.any():\n                continue\n                \n            # Get predicted boxes\n            pred_boxes = prediction[b, obj_mask]\n            \n            # Get box attributes\n            box_attr = torch.zeros_like(pred_boxes)\n            \n            # Get grid positions\n            grid_indices = obj_mask.nonzero()\n            anchor_idx, grid_y, grid_x = grid_indices[:, 0], grid_indices[:, 1], grid_indices[:, 2]\n            \n            # Apply anchor box offsets and scale\n            box_attr[:, 0] = (torch.sigmoid(pred_boxes[:, 0]) + grid_x) / grid_size\n            box_attr[:, 1] = (torch.sigmoid(pred_boxes[:, 1]) + grid_y) / grid_size\n            \n            # Width and height\n            anchor_w = torch.tensor([a[0] for a in anchors[i]], device=device)[anchor_idx]\n            anchor_h = torch.tensor([a[1] for a in anchors[i]], device=device)[anchor_idx]\n            \n            box_attr[:, 2] = torch.exp(pred_boxes[:, 2]) * anchor_w\n            box_attr[:, 3] = torch.exp(pred_boxes[:, 3]) * anchor_h\n            \n            # Convert to corner coordinates (x1, y1, x2, y2)\n            box_attr[:, 0:2] -= box_attr[:, 2:4] / 2\n            box_attr[:, 2:4] += box_attr[:, 0:2]\n            \n            # Scale to image size\n            box_attr[:, 0:4] *= img_size\n            \n            # Add confidence scores\n            box_attr[:, 4] = pred_boxes[:, 4]\n            \n            # Add class with highest probability\n            box_attr[:, 5] = pred_boxes[:, 5:].argmax(1)\n            \n            all_boxes.append(box_attr)\n    \n    # Combine all boxes from different scales\n    if len(all_boxes) > 0:\n        return torch.cat(all_boxes, dim=0)\n    else:\n        return torch.zeros((0, 6), device=device)\n\n\n# Non-Maximum Suppression to remove overlapping boxes\ndef non_max_suppression(boxes, iou_threshold=0.5):\n    \"\"\"\n    Apply Non-Maximum Suppression to remove overlapping boxes.\n    \n    Args:\n        boxes: Tensor of shape (N, 6) with each box as [x1, y1, x2, y2, conf, class_idx]\n        iou_threshold: IoU threshold for NMS\n    \n    Returns:\n        Tensor of shape (M, 6) with M <= N\n    \"\"\"\n    if boxes.size(0) == 0:\n        return boxes\n    \n    # Sort by confidence\n    _, order = boxes[:, 4].sort(descending=True)\n    boxes = boxes[order]\n    \n    keep = []\n    \n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    area = (x2 - x1) * (y2 - y1)\n    \n    while boxes.size(0) > 0:\n        i = 0  # Keep the box with highest confidence\n        keep.append(order[i])\n        \n        if boxes.size(0) == 1:\n            break\n            \n        # Get IoU of the box with highest confidence with all other boxes\n        xx1 = torch.max(boxes[0, 0], boxes[1:, 0])\n        yy1 = torch.max(boxes[0, 1], boxes[1:, 1])\n        xx2 = torch.min(boxes[0, 2], boxes[1:, 2])\n        yy2 = torch.min(boxes[0, 3], boxes[1:, 3])\n        \n        w = torch.clamp(xx2 - xx1, min=0)\n        h = torch.clamp(yy2 - yy1, min=0)\n        \n        inter = w * h\n        iou = inter / (area[0] + area[1:] - inter)\n        \n        # Keep boxes with IoU below threshold\n        inds = torch.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n        boxes = boxes[inds + 1]\n        area = area[inds + 1]\n    \n    return torch.stack([boxes[i] for i in keep])\n\n\n# YOLO loss function\nclass YOLOLoss(nn.Module):\n    def __init__(self, anchors, grid_sizes, num_classes=NUM_CLASSES):\n        super(YOLOLoss, self).__init__()\n        self.anchors = anchors\n        self.grid_sizes = grid_sizes\n        self.num_classes = num_classes\n        self.lambda_coord = 5.0  # Weight for box coordinate loss\n        self.lambda_noobj = 0.5  # Weight for no-object confidence loss\n        self.mse_loss = nn.MSELoss(reduction='mean')  # Changed to mean\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='mean')  # Changed to BCEWithLogitsLoss for stability\n    \n    def forward(self, predictions, targets):\n        \"\"\"\n        Calculate YOLO loss with improved stability.\n        \n        Args:\n            predictions: List of tensors from model output\n            targets: List of tensors with ground truth boxes for each image\n                     Each box is [class_idx, x_center, y_center, width, height]\n        \n        Returns:\n            Total loss\n        \"\"\"\n        batch_size = len(targets)\n        if batch_size == 0:\n            return torch.tensor(0.0, device=device, requires_grad=True)\n            \n        total_loss = 0\n        coord_loss, obj_loss, noobj_loss, cls_loss = 0, 0, 0, 0\n        num_scale_losses = 0\n        \n        # Process each scale separately\n        for scale_idx, (pred, grid_size) in enumerate(zip(predictions, self.grid_sizes)):\n            batch_size = pred.size(0)\n            num_anchors = len(self.anchors[scale_idx])\n            \n            # Reshape predictions\n            pred = pred.view(batch_size, num_anchors, self.num_classes + 5, grid_size, grid_size)\n            pred = pred.permute(0, 1, 3, 4, 2).contiguous()\n            \n            # Create target tensors\n            obj_mask = torch.zeros(batch_size, num_anchors, grid_size, grid_size, device=device)\n            noobj_mask = torch.ones(batch_size, num_anchors, grid_size, grid_size, device=device)\n            tx = torch.zeros(batch_size, num_anchors, grid_size, grid_size, device=device)\n            ty = torch.zeros(batch_size, num_anchors, grid_size, grid_size, device=device)\n            tw = torch.zeros(batch_size, num_anchors, grid_size, grid_size, device=device)\n            th = torch.zeros(batch_size, num_anchors, grid_size, grid_size, device=device)\n            tcls = torch.zeros(batch_size, num_anchors, grid_size, grid_size, self.num_classes, device=device)\n            \n            target_count = 0  # Count of valid targets assigned at this scale\n            \n            # Process each image in the batch\n            for b in range(batch_size):\n                if b >= len(targets) or targets[b].size(0) == 0:\n                    continue\n                    \n                this_img_targets = targets[b]\n                \n                # Process each target box\n                for target_box in this_img_targets:\n                    # Skip invalid classes\n                    class_idx = target_box[0].long().item()\n                    if class_idx < 0 or class_idx >= self.num_classes:\n                        continue\n                        \n                    # Convert target box to grid scale\n                    gx = target_box[1] * grid_size\n                    gy = target_box[2] * grid_size\n                    gw = target_box[3] * grid_size\n                    gh = target_box[4] * grid_size\n                    \n                    # Grid cell indices (center of box)\n                    gi = int(gx)\n                    gj = int(gy)\n                    \n                    # Check if grid indices are within bounds\n                    if gi >= grid_size or gj >= grid_size or gi < 0 or gj < 0:\n                        continue\n                    \n                    # Calculate best anchor based on IoU\n                    target_box_tensor = torch.tensor([0, 0, target_box[3], target_box[4]], device=device)\n                    anchor_ious = []\n                    \n                    for anchor_idx in range(num_anchors):\n                        anchor_box = torch.tensor(\n                            [0, 0, self.anchors[scale_idx][anchor_idx][0], self.anchors[scale_idx][anchor_idx][1]],\n                            device=device\n                        )\n                        anchor_ious.append(bbox_iou(anchor_box, target_box_tensor, x1y1x2y2=False))\n                    \n                    anchor_ious = torch.tensor(anchor_ious, device=device)\n                    best_anchor = torch.argmax(anchor_ious).item()\n                    \n                    # Only assign target if IoU is good enough\n                    if anchor_ious[best_anchor] > 0.2:\n                        # Mark grid cell as having an object\n                        obj_mask[b, best_anchor, gj, gi] = 1\n                        noobj_mask[b, best_anchor, gj, gi] = 0\n                        \n                        # Box coordinates - the fractions for tx, ty\n                        tx[b, best_anchor, gj, gi] = gx - gi\n                        ty[b, best_anchor, gj, gi] = gy - gj\n                        \n                        # Width and height - log scale (same as YOLOv3)\n                        tw[b, best_anchor, gj, gi] = torch.log(\n                            gw / self.anchors[scale_idx][best_anchor][0] + 1e-16\n                        )\n                        th[b, best_anchor, gj, gi] = torch.log(\n                            gh / self.anchors[scale_idx][best_anchor][1] + 1e-16\n                        )\n                        \n                        # Class probability\n                        tcls[b, best_anchor, gj, gi, class_idx] = 1\n                        \n                        target_count += 1\n            \n            # Skip loss calculation if no targets assigned at this scale\n            if target_count == 0:\n                continue\n                \n            num_scale_losses += 1\n            \n            # Mask for cells with assigned targets\n            obj_count = obj_mask.sum()\n            noobj_count = noobj_mask.sum()\n            \n            # Box coordinate loss\n            box_pred_xy = torch.sigmoid(pred[..., 0:2][obj_mask == 1])\n            box_target_xy = torch.stack([\n                tx[obj_mask == 1],\n                ty[obj_mask == 1]\n            ], dim=1)\n            \n            if box_pred_xy.numel() > 0:  # Only calculate if we have predictions\n                coord_loss += self.mse_loss(box_pred_xy, box_target_xy)\n                \n                # Width and height loss\n                box_pred_wh = pred[..., 2:4][obj_mask == 1]\n                box_target_wh = torch.stack([\n                    tw[obj_mask == 1],\n                    th[obj_mask == 1]\n                ], dim=1)\n                \n                coord_loss += self.mse_loss(box_pred_wh, box_target_wh)\n            \n            # Objectness loss (using BCE with logits for stability)\n            obj_loss += self.bce_loss(\n                pred[..., 4][obj_mask == 1],\n                torch.ones_like(pred[..., 4][obj_mask == 1])\n            )\n            \n            # No-object loss (using BCE with logits for stability)\n            # Limit the number of no-object examples to balance the loss\n            if noobj_count > 0:\n                # Balance: use at most 3x the number of objects\n                max_noobj = min(int(obj_count * 3), int(noobj_count))\n                if max_noobj > 0:\n                    noobj_mask_balanced = (noobj_mask.flatten() > 0).nonzero().squeeze()\n                    # Randomly sample indices if we have too many\n                    if noobj_mask_balanced.numel() > max_noobj:\n                        idx = torch.randperm(noobj_mask_balanced.numel())[:max_noobj]\n                        noobj_mask_balanced = noobj_mask_balanced[idx]\n                    \n                    flat_pred = pred[..., 4].flatten()\n                    noobj_pred = flat_pred[noobj_mask_balanced]\n                    \n                    noobj_loss += self.bce_loss(\n                        noobj_pred, \n                        torch.zeros_like(noobj_pred)\n                    )\n            \n            # Class loss\n            if obj_count > 0:\n                cls_pred = pred[..., 5:][obj_mask == 1]\n                cls_target = tcls[obj_mask == 1]\n                \n                if cls_pred.numel() > 0:\n                    cls_loss += self.bce_loss(cls_pred, cls_target)\n        \n        # Combine losses with weights\n        if num_scale_losses > 0:\n            total_loss = (\n                self.lambda_coord * coord_loss +\n                obj_loss +\n                self.lambda_noobj * noobj_loss +\n                cls_loss\n            ) / num_scale_losses\n        else:\n            # Return a small loss to prevent NaN gradients if no targets\n            total_loss = torch.tensor(0.1, device=device, requires_grad=True)\n        \n        return total_loss\n\n\n# Improved NMS implementation\ndef improved_nms(boxes, iou_threshold=0.5):\n    \"\"\"\n    Perform Non-Maximum Suppression on the bounding boxes.\n    \n    Args:\n        boxes: Tensor of shape (N, 6) where each row is\n               (x1, y1, x2, y2, confidence, class_idx)\n        iou_threshold: IoU threshold for boxes to be considered overlapping\n        \n    Returns:\n        Tensor of shape (M, 6) where M <= N, containing filtered boxes\n    \"\"\"\n    # If no boxes, return empty tensor\n    if boxes.numel() == 0:\n        return torch.zeros((0, 6), device=boxes.device)\n    \n    # Make sure boxes is 2D tensor with shape (N, 6)\n    if len(boxes.shape) == 1:\n        # If we have a single box, reshape to (1, 6)\n        boxes = boxes.unsqueeze(0)\n    \n    # Extract coordinates, scores, and class indices\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    scores = boxes[:, 4]\n    class_idxs = boxes[:, 5]\n    \n    # Calculate areas for all boxes\n    areas = (x2 - x1) * (y2 - y1)\n    \n    # Sort boxes by confidence scores (high to low)\n    _, order = scores.sort(descending=True)\n    \n    keep_boxes = []\n    \n    while order.numel() > 0:\n        # Pick the box with highest confidence score\n        i = order[0].item()\n        \n        # Add it to the list of kept boxes\n        box_to_keep = boxes[i]\n        \n        # Ensure box_to_keep is a 1D tensor\n        if len(box_to_keep.shape) > 1:\n            box_to_keep = box_to_keep.squeeze(0)\n        \n        keep_boxes.append(box_to_keep)\n        \n        # If this was the last box, break\n        if order.numel() == 1:\n            break\n        \n        # Calculate IoU of the picked box with the rest\n        xx1 = torch.max(x1[i], x1[order[1:]])\n        yy1 = torch.max(y1[i], y1[order[1:]])\n        xx2 = torch.min(x2[i], x2[order[1:]])\n        yy2 = torch.min(y2[i], y2[order[1:]])\n        \n        # Calculate intersection area\n        width = torch.clamp(xx2 - xx1, min=0)\n        height = torch.clamp(yy2 - yy1, min=0)\n        intersection = width * height\n        \n        # Calculate IoU\n        union = areas[i] + areas[order[1:]] - intersection\n        iou = intersection / union\n        \n        # Keep boxes with IoU less than threshold and same class\n        same_class = class_idxs[order[1:]] == class_idxs[i]\n        below_thresh = iou <= iou_threshold\n        mask = below_thresh | ~same_class\n        \n        # Update order indices\n        order = order[1:][mask]\n    \n    # Stack kept boxes into a single tensor\n    # First ensure all tensors have the same shape (1D with 6 elements)\n    for i in range(len(keep_boxes)):\n        if len(keep_boxes[i].shape) > 1:\n            keep_boxes[i] = keep_boxes[i].squeeze(0)\n    \n    if keep_boxes:\n        return torch.stack(keep_boxes)\n    else:\n        return torch.zeros((0, 6), device=boxes.device)\n\n\n# Efficient IoU calculation for batched boxes\ndef box_iou(box1, box2):\n    \"\"\"\n    Calculate IoU between two sets of boxes.\n    \n    Args:\n        box1: Tensor of shape (N, 4)\n        box2: Tensor of shape (M, 4)\n    \n    Returns:\n        IoU: Tensor of shape (N, M)\n    \"\"\"\n    # Get box coordinates\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n    \n    # Get the coordinates of intersecting rectangles\n    inter_x1 = torch.max(b1_x1.unsqueeze(1), b2_x1.unsqueeze(0))\n    inter_y1 = torch.max(b1_y1.unsqueeze(1), b2_y1.unsqueeze(0))\n    inter_x2 = torch.min(b1_x2.unsqueeze(1), b2_x2.unsqueeze(0))\n    inter_y2 = torch.min(b1_y2.unsqueeze(1), b2_y2.unsqueeze(0))\n    \n    # Calculate intersection area\n    inter_w = torch.clamp(inter_x2 - inter_x1, min=0)\n    inter_h = torch.clamp(inter_y2 - inter_y1, min=0)\n    inter_area = inter_w * inter_h\n    \n    # Calculate union area\n    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n    union_area = b1_area.unsqueeze(1) + b2_area.unsqueeze(0) - inter_area\n    \n    # Calculate IoU\n    iou = inter_area / (union_area + 1e-16)\n    \n    return iou\n\n\n# Calculate IoU between two bounding boxes\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    \"\"\"\n    Calculate IoU between two boxes.\n    \n    Args:\n        box1: Tensor of shape (4,)\n        box2: Tensor of shape (4,)\n        x1y1x2y2: If True, box format is [x1, y1, x2, y2], otherwise [x, y, w, h]\n    \n    Returns:\n        IoU score\n    \"\"\"\n    if not x1y1x2y2:\n        # Transform from [x, y, w, h] to [x1, y1, x2, y2]\n        b1_x1, b1_y1 = box1[0] - box1[2] / 2, box1[1] - box1[3] / 2\n        b1_x2, b1_y2 = box1[0] + box1[2] / 2, box1[1] + box1[3] / 2\n        b2_x1, b2_y1 = box2[0] - box2[2] / 2, box2[1] - box2[3] / 2\n        b2_x2, b2_y2 = box2[0] + box2[2] / 2, box2[1] + box2[3] / 2\n    else:\n        # Get box coordinates\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n    \n    # Get intersection area\n    inter_x1 = torch.max(b1_x1, b2_x1)\n    inter_y1 = torch.max(b1_y1, b2_y1)\n    inter_x2 = torch.min(b1_x2, b2_x2)\n    inter_y2 = torch.min(b1_y2, b2_y2)\n    \n    # Calculate intersection area\n    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n    \n    # Calculate union area\n    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n    union_area = b1_area + b2_area - inter_area\n    \n    # Calculate IoU\n    iou = inter_area / (union_area + 1e-16)\n    \n    return iou\n\n\n# Calculate mAP for evaluation\n# Calculate mAP for evaluation\n# Calculate mAP for evaluation\ndef calculate_mAP(pred_boxes, true_boxes, num_classes=NUM_CLASSES, iou_threshold=0.5):\n    \"\"\"\n    Calculate mean Average Precision for object detection.\n    \n    Args:\n        pred_boxes: List of tensors, each with shape [num_boxes, 6] (x1,y1,x2,y2,conf,class)\n        true_boxes: List of tensors, each with shape [num_boxes, 5] (class,x,y,w,h)\n        num_classes: Number of classes\n        iou_threshold: IoU threshold for a detection to be considered correct\n        \n    Returns:\n        mAP score\n    \"\"\"\n    # Initialize counters and storage\n    average_precisions = []\n    \n    # Track if we have any valid predictions\n    has_valid_predictions = False\n    \n    # Debug counters\n    total_predictions = 0\n    valid_predictions = 0\n    \n    # Process each class\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n        \n        # Get all detections and ground truths for this class\n        for img_idx in range(len(pred_boxes)):\n            # Get predictions for this class (if any exist for this image)\n            if img_idx < len(pred_boxes) and pred_boxes[img_idx].size(0) > 0:\n                # Extract boxes for this class\n                pred = pred_boxes[img_idx]\n                mask = (pred[:, 5] == c)\n                total_predictions += mask.sum().item()\n                \n                if mask.sum() > 0:\n                    valid_predictions += mask.sum().item()\n                    has_valid_predictions = True\n                    \n                    # Store detections\n                    for box, conf in zip(pred[mask, :4], pred[mask, 4]):\n                        detections.append({\n                            'image_id': img_idx,\n                            'confidence': conf.item(),\n                            'bbox': box.cpu().numpy()\n                        })\n            \n            # Get ground truths for this class (if any exist for this image)\n            if img_idx < len(true_boxes) and true_boxes[img_idx].size(0) > 0:\n                gt = true_boxes[img_idx]\n                \n                # Check if gt is a 1D tensor (single box case)\n                if gt.dim() == 1:\n                    # Single box case - check if this box is for class c\n                    if gt[0] == c:\n                        # Convert from normalized [x_center, y_center, width, height] to absolute [x1, y1, x2, y2]\n                        x, y, w, h = gt[1:].cpu().numpy()\n                        x1 = (x - w / 2) * IMG_SIZE\n                        y1 = (y - h / 2) * IMG_SIZE\n                        x2 = (x + w / 2) * IMG_SIZE\n                        y2 = (y + h / 2) * IMG_SIZE\n                        ground_truths.append({\n                            'image_id': img_idx,\n                            'bbox': np.array([x1, y1, x2, y2])\n                        })\n                else:\n                    # Multiple boxes case\n                    class_mask = (gt[:, 0] == c)\n                    if class_mask.sum() > 0:\n                        for box in gt[class_mask, 1:]:\n                            # Convert from normalized [x_center, y_center, width, height] to absolute [x1, y1, x2, y2]\n                            x, y, w, h = box.cpu().numpy()\n                            x1 = (x - w / 2) * IMG_SIZE\n                            y1 = (y - h / 2) * IMG_SIZE\n                            x2 = (x + w / 2) * IMG_SIZE\n                            y2 = (y + h / 2) * IMG_SIZE\n                            ground_truths.append({\n                                'image_id': img_idx,\n                                'bbox': np.array([x1, y1, x2, y2])\n                            })\n        \n        # Skip class if no ground truths\n        if len(ground_truths) == 0:\n            continue\n            \n        # Debug print\n        print(f\"Class {c}: {len(detections)} detections, {len(ground_truths)} ground truths\")\n        \n        # Sort detections by confidence\n        detections.sort(key=lambda x: x['confidence'], reverse=True)\n        \n        # Initialize counters for precision/recall calculation\n        TP = np.zeros(len(detections))\n        FP = np.zeros(len(detections))\n        \n        # Track which ground truths were used (avoid double counting)\n        # For each image, create a set to track used ground truth boxes\n        gt_used = {img_idx: set() for img_idx in range(len(true_boxes))}\n        \n        # Process each detection\n        for d_idx, detection in enumerate(detections):\n            img_id = detection['image_id']\n            \n            # Get ground truths for this image\n            gt_this_img = [i for i, gt in enumerate(ground_truths) if gt['image_id'] == img_id]\n            \n            # No ground truths for this image\n            if len(gt_this_img) == 0:\n                FP[d_idx] = 1\n                continue\n                \n            # Get detection bounding box\n            d_bbox = detection['bbox']\n            \n            # Find best matching ground truth\n            max_iou = -float('inf')\n            max_gt_idx = -1\n            \n            for gt_idx in gt_this_img:\n                # Skip if this ground truth was already used\n                if gt_idx in gt_used[img_id]:\n                    continue\n                    \n                # Calculate IoU\n                gt_bbox = ground_truths[gt_idx]['bbox']\n                iou = calculate_iou(d_bbox, gt_bbox)\n                \n                if iou > max_iou:\n                    max_iou = iou\n                    max_gt_idx = gt_idx\n                    \n            # Check if detection matches any ground truth\n            if max_iou >= iou_threshold and max_gt_idx != -1:\n                # Mark this ground truth as used\n                gt_used[img_id].add(max_gt_idx)\n                TP[d_idx] = 1\n            else:\n                FP[d_idx] = 1\n                \n        # Calculate precision and recall\n        TP_cumsum = np.cumsum(TP)\n        FP_cumsum = np.cumsum(FP)\n        recalls = TP_cumsum / len(ground_truths)\n        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + 1e-16)  # Add small epsilon to avoid division by zero\n        \n        # Add sentinel values to start the precision-recall curve\n        precisions = np.concatenate(([1], precisions))\n        recalls = np.concatenate(([0], recalls))\n        \n        # Calculate average precision\n        AP = np.trapz(precisions, recalls)\n        average_precisions.append(AP)\n        print(f\"Class {c} AP: {AP:.4f}\")\n    \n    # Calculate mean AP\n    if len(average_precisions) > 0:\n        mAP = np.mean(average_precisions)\n        print(f\"Total valid predictions: {valid_predictions}/{total_predictions}\")\n        print(f\"mAP: {mAP:.4f} (across {len(average_precisions)} classes)\")\n        return mAP\n    else:\n        print(\"No valid predictions or ground truths found!\")\n        return 0.0\n\n\n# Helper function to calculate IoU for two boxes\ndef calculate_iou(box1, box2):\n    \"\"\"\n    Calculate IoU between two boxes in format [x1, y1, x2, y2].\n    \"\"\"\n    # Determine coordinates of intersection\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    # Calculate area of intersection\n    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n    \n    # Calculate area of both bounding boxes\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    \n    # Calculate union\n    union = box1_area + box2_area - intersection\n    \n    # Return IoU\n    return intersection / union if union > 0 else 0\n\n\n# Set up data transformations\ndef get_transforms(train=True):\n    \"\"\"\n    Get transformations for images.\n    \"\"\"\n    if train:\n        return transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n            transforms.RandomRotation(10),\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n\n# Collate function for data loader\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function for dataloader to handle variable sized targets.\n    \"\"\"\n    images, targets = zip(*batch)\n    \n    # Stack images\n    images = torch.stack(images)\n    \n    # Return images and targets\n    return images, targets\n\n\n# Training function\ndef train_model(model, train_loader, val_loader, epochs=5):\n    \"\"\"\n    Improved training function with gradient clipping and better learning rate.\n    \n    Args:\n        model: Model to train\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        epochs: Number of epochs to train for\n    \n    Returns:\n        Trained model and training history\n    \"\"\"\n    # Move model to device\n    model = model.to(device)\n    \n    # Loss function\n    criterion = YOLOLoss(ANCHORS, GRID_SIZES, NUM_CLASSES).to(device)\n    \n    # Optimizer with lower learning rate and weight decay\n    optimizer = optim.AdamW(\n        model.parameters(), \n        lr=1e-5,  # Lower initial learning rate \n        weight_decay=1e-4  # Add weight decay to prevent overfitting\n    )\n    \n    # Learning rate scheduler with warmup\n    def lr_lambda(epoch):\n        # Warm up for first 3 epochs\n        if epoch < 3:\n            return (epoch + 1) / 3\n        # Then follow cosine schedule\n        return 0.5 * (1 + np.cos((epoch - 3) / (epochs - 3) * np.pi))\n    \n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_mAP': [],\n        'learning_rates': []\n    }\n    \n    # Start training\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        \n        # Progress bar\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n        batch_count = 0\n        \n        for images, targets in progress_bar:\n            # Skip batch if all targets are empty\n            if all(len(t) == 0 for t in targets):\n                continue\n                \n            # Move to device\n            images = images.to(device)\n            targets = [t.to(device) for t in targets]\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Calculate loss\n            loss = criterion(outputs, targets)\n            \n            # Skip bad batches with extremely high loss\n            if loss.item() > 1e6:\n                print(f\"Skipping batch with loss: {loss.item():.2e}\")\n                continue\n            \n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # Clip gradients to prevent exploding gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=35)\n            \n            optimizer.step()\n            \n            # Update progress bar\n            batch_loss = loss.item() / images.size(0)  # Normalize by batch size\n            epoch_loss += batch_loss\n            batch_count += 1\n            \n            lr = optimizer.param_groups[0]['lr']\n            progress_bar.set_postfix({\n                'loss': batch_loss,\n                'lr': lr\n            })\n        \n        # Average training loss for the epoch\n        avg_train_loss = epoch_loss / batch_count if batch_count > 0 else float('inf')\n        history['train_loss'].append(avg_train_loss)\n        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n\n       # VALIDATION LOOP - update this part\n        model.eval()\n        val_loss = 0\n        val_batch_count = 0\n        \n        # Store predictions and ground truth for mAP calculation\n        all_pred_boxes = []\n        all_true_boxes = []\n        \n        with torch.no_grad():\n            for images, targets in tqdm(val_loader, desc='Validation'):\n                # Skip batch if all targets are empty\n                if all(len(t) == 0 for t in targets):\n                    continue\n                \n                # Move to device\n                images = images.to(device)\n                targets = [t.to(device) for t in targets]\n                \n                # Forward pass\n                outputs = model(images)\n                \n                # Calculate loss\n                loss = criterion(outputs, targets)\n                batch_loss = loss.item() / images.size(0)\n                val_loss += batch_loss\n                val_batch_count += 1\n                \n                # Process predictions\n                for i in range(images.size(0)):\n                    # Get raw predictions for this image\n                    pred_boxes = []\n                    \n                    # Process each scale\n                    for scale_idx, output in enumerate(outputs):\n                        # Process this scale's predictions\n                        batch, _, grid_size, _ = output.shape\n                        num_anchors = len(ANCHORS[scale_idx])\n                        num_classes = NUM_CLASSES\n                        \n                        # Reshape to [batch, anchors, grid, grid, 5+classes]\n                        prediction = output.view(batch, num_anchors, 5+num_classes, grid_size, grid_size)\n                        prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()\n                        \n                        # Apply sigmoid to confidence and class scores\n                        prediction[..., 4:] = torch.sigmoid(prediction[..., 4:])\n                        \n                        # Process this image\n                        img_pred = prediction[i]\n                        \n                        # Get boxes with confidence > threshold (0.1 for validation)\n                        conf_mask = img_pred[..., 4] > 0.1\n                        if not conf_mask.any():\n                            continue\n                            \n                        # Get predicted boxes\n                        pred = img_pred[conf_mask]\n                        \n                        # Get grid positions\n                        grid_indices = conf_mask.nonzero()\n                        anchor_idx, grid_y, grid_x = grid_indices[:, 0], grid_indices[:, 1], grid_indices[:, 2]\n                        \n                        # Convert to absolute coordinates\n                        boxes = torch.zeros((pred.size(0), 6), device=device)\n                        boxes[:, 0] = (torch.sigmoid(pred[:, 0]) + grid_x) / grid_size * IMG_SIZE  # x center\n                        boxes[:, 1] = (torch.sigmoid(pred[:, 1]) + grid_y) / grid_size * IMG_SIZE  # y center\n                        \n                        # Width and height\n                        anchor_w = torch.tensor([a[0] for a in ANCHORS[scale_idx]], device=device)[anchor_idx]\n                        anchor_h = torch.tensor([a[1] for a in ANCHORS[scale_idx]], device=device)[anchor_idx]\n                        boxes[:, 2] = torch.exp(pred[:, 2]) * anchor_w * IMG_SIZE  # width\n                        boxes[:, 3] = torch.exp(pred[:, 3]) * anchor_h * IMG_SIZE  # height\n                        \n                        # Convert to corner format\n                        boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1\n                        boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1\n                        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]      # x2\n                        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]      # y2\n                        \n                        # Confidence and class\n                        boxes[:, 4] = pred[:, 4]  # confidence\n                        boxes[:, 5] = pred[:, 5:].argmax(1)  # class index\n                        \n                        pred_boxes.append(boxes)\n                    \n                    # Combine predictions from all scales\n                    if pred_boxes:\n                        img_boxes = torch.cat(pred_boxes, dim=0)\n                        # Apply NMS\n                        img_boxes = improved_nms(img_boxes, iou_threshold=0.5)\n                        all_pred_boxes.append(img_boxes)\n                    else:\n                        all_pred_boxes.append(torch.zeros((0, 6), device=device))\n                \n                # Add targets to list - deep copy to avoid reference issues\n                for target in targets:\n                    all_true_boxes.append(target.clone())\n        \n        # Average validation loss\n        avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n        history['val_loss'].append(avg_val_loss)\n        \n        # Calculate mAP\n        print(\"\\nCalculating validation mAP...\")\n        if len(all_pred_boxes) > 0 and len(all_true_boxes) > 0:\n            val_mAP = calculate_mAP(all_pred_boxes, all_true_boxes)\n            history['val_mAP'].append(val_mAP)\n        else:\n            val_mAP = 0\n            history['val_mAP'].append(0)\n            print(\"No valid predictions or ground truths for mAP calculation!\")\n        \n        # Update learning rate scheduler\n        scheduler.step()\n        \n        # Print metrics\n        print(f'Epoch {epoch+1}/{epochs}:')\n        print(f'  Train Loss: {avg_train_loss:.6f}')\n        print(f'  Val Loss: {avg_val_loss:.6f}')\n        print(f'  Val mAP: {val_mAP:.6f}')\n        print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n        \n        # Early stopping if loss is NaN or too high\n        if np.isnan(avg_train_loss) or avg_train_loss > 1e5:\n            print(\"Stopping early due to unstable training\")\n            break\n        \n        # Save model checkpoint\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'val_mAP': val_mAP\n        }, f'yolo_densenet_checkpoint_epoch{epoch+1}.pth')\n    \n    return model, history\n\n# Visualization function for debugging\ndef visualize_predictions(model, image_path, conf_thresh=0.25):\n    \"\"\"\n    Visualize predictions on a single image.\n    \n    Args:\n        model: Trained model\n        image_path: Path to image\n        conf_thresh: Confidence threshold for detections\n    \"\"\"\n    # Load and transform image\n    image = Image.open(image_path).convert('RGB')\n    orig_img = np.array(image)\n    \n    # Apply transformations\n    transform = get_transforms(train=False)\n    tensor_img = transform(image).unsqueeze(0).to(device)\n    \n    # Get predictions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(tensor_img)\n        pred_boxes = process_predictions(outputs, ANCHORS, conf_thresh=conf_thresh)\n        pred_boxes = improved_nms(pred_boxes)\n    \n    # Draw bounding boxes\n    img_with_boxes = orig_img.copy()\n    \n    if pred_boxes.size(0) > 0:\n        # Scale coordinates to original image size\n        scale_x = orig_img.shape[1] / IMG_SIZE\n        scale_y = orig_img.shape[0] / IMG_SIZE\n        \n        for box in pred_boxes:\n            x1, y1, x2, y2 = box[:4]\n            x1 = int(x1.item() * scale_x)\n            y1 = int(y1.item() * scale_y)\n            x2 = int(x2.item() * scale_x)\n            y2 = int(y2.item() * scale_y)\n            \n            conf = box[4].item()\n            cls_idx = int(box[5].item())\n            \n            # Draw rectangle\n            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            \n            # Add label\n            label = f'{COCO_CLASSES[cls_idx]}: {conf:.2f}'\n            cv2.putText(img_with_boxes, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n    \n    # Display image\n    plt.figure(figsize=(12, 8))\n    plt.imshow(img_with_boxes)\n    plt.axis('off')\n    plt.show()\n\n\n# Demo function to detect objects in a test image\ndef demo(model_path, image_path, conf_thresh=0.25):\n    \"\"\"\n    Demo function to load a trained model and detect objects in an image.\n    \n    Args:\n        model_path: Path to trained model checkpoint\n        image_path: Path to test image\n        conf_thresh: Confidence threshold for detections\n    \"\"\"\n    # Load model\n    model = DenseNetYOLO(num_classes=NUM_CLASSES)\n    checkpoint = torch.load(model_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    # Visualize predictions\n    visualize_predictions(model, image_path, conf_thresh)\n\n\n# Main function to run the training pipeline\ndef main():\n    # Define paths to dataset\n    coco_img_dir = '/kaggle/input/coco-2017-dataset/coco2017'\n    coco_ann_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations'\n    \n    # Create datasets\n    train_dataset = COCODataset(\n        img_dir=coco_img_dir + '/train2017',\n        annotations_file=coco_ann_file + '/instances_train2017.json',\n        transform=get_transforms(train=True)\n    )\n    \n    val_dataset = COCODataset(\n        img_dir=coco_img_dir + '/val2017',\n        annotations_file=coco_ann_file + '/instances_val2017.json',\n        transform=get_transforms(train=False)\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=64,  # Adjust based on your GPU memory\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=64,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    # Create model\n    model = DenseNetYOLO(num_classes=NUM_CLASSES)\n    \n    # Train model\n    trained_model, history = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        epochs=5  # Adjust as needed\n    )\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['val_mAP'], label='Val mAP')\n    plt.xlabel('Epoch')\n    plt.ylabel('mAP')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.show()\n    \n    # Demo on a test image\n    \n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:38:27.660967Z","iopub.execute_input":"2025-05-11T12:38:27.661350Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:02<00:00, 15.6MB/s]\nEpoch 1/5: 100%|██████████| 1849/1849 [1:12:48<00:00,  2.36s/it, loss=0.22, lr=3.33e-6]  \nValidation: 100%|██████████| 79/79 [45:29<00:00, 34.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating validation mAP...\nClass 0: 74085 detections, 11004 ground truths\nClass 0 AP: 0.0000\nClass 1: 77584 detections, 316 ground truths\nClass 1 AP: 0.0000\nClass 2: 4741 detections, 1932 ground truths\nClass 2 AP: 0.0000\nClass 3: 15120 detections, 371 ground truths\nClass 3 AP: 0.0000\nClass 4: 85155 detections, 143 ground truths\nClass 4 AP: 0.0000\nClass 5: 35067 detections, 285 ground truths\nClass 5 AP: 0.0000\nClass 6: 172462 detections, 190 ground truths\nClass 6 AP: 0.0000\nClass 7: 72268 detections, 415 ground truths\nClass 7 AP: 0.0000\nClass 8: 137479 detections, 430 ground truths\nClass 8 AP: 0.0000\nClass 9: 58569 detections, 637 ground truths\nClass 9 AP: 0.0000\nClass 10: 22027 detections, 101 ground truths\nClass 10 AP: 0.0000\nClass 11: 121712 detections, 75 ground truths\nClass 11 AP: 0.0000\nClass 12: 22822 detections, 60 ground truths\nClass 12 AP: 0.0000\nClass 13: 3657 detections, 413 ground truths\nClass 13 AP: 0.0000\nClass 14: 35766 detections, 440 ground truths\nClass 14 AP: 0.0000\nClass 15: 3522 detections, 202 ground truths\nClass 15 AP: 0.0000\nClass 16: 14417 detections, 218 ground truths\nClass 16 AP: 0.0000\nClass 17: 25100 detections, 273 ground truths\nClass 17 AP: 0.0000\nClass 18: 36540 detections, 361 ground truths\nClass 18 AP: 0.0000\nClass 19: 161636 detections, 380 ground truths\nClass 19 AP: 0.0000\nClass 20: 151250 detections, 255 ground truths\nClass 20 AP: 0.0000\nClass 21: 8005 detections, 71 ground truths\nClass 21 AP: 0.0000\nClass 22: 27299 detections, 268 ground truths\nClass 22 AP: 0.0000\nClass 23: 52031 detections, 232 ground truths\nClass 23 AP: 0.0000\nClass 24: 45965 detections, 371 ground truths\nClass 24 AP: 0.0000\nClass 25: 26409 detections, 413 ground truths\nClass 25 AP: 0.0000\nClass 26: 53312 detections, 540 ground truths\nClass 26 AP: 0.0000\nClass 27: 80189 detections, 254 ground truths\nClass 27 AP: 0.0000\nClass 28: 13890 detections, 303 ground truths\nClass 28 AP: 0.0000\nClass 29: 15664 detections, 115 ground truths\nClass 29 AP: 0.0000\nClass 30: 141330 detections, 241 ground truths\nClass 30 AP: 0.0000\nClass 31: 39136 detections, 69 ground truths\nClass 31 AP: 0.0000\nClass 32: 8107 detections, 263 ground truths\nClass 32 AP: 0.0000\nClass 33: 38979 detections, 336 ground truths\nClass 33 AP: 0.0000\nClass 34: 8390 detections, 146 ground truths\nClass 34 AP: 0.0000\nClass 35: 2658 detections, 148 ground truths\nClass 35 AP: 0.0000\nClass 36: 11719 detections, 179 ground truths\nClass 36 AP: 0.0000\nClass 37: 67516 detections, 269 ground truths\nClass 37 AP: 0.0000\nClass 38: 28143 detections, 225 ground truths\nClass 38 AP: 0.0000\nClass 39: 8504 detections, 1025 ground truths\nClass 39 AP: 0.0000\nClass 40: 207981 detections, 343 ground truths\nClass 40 AP: 0.0000\nClass 41: 16298 detections, 899 ground truths\nClass 41 AP: 0.0000\nClass 42: 18437 detections, 215 ground truths\nClass 42 AP: 0.0000\nClass 43: 53569 detections, 326 ground truths\nClass 43 AP: 0.0000\nClass 44: 727878 detections, 253 ground truths\nClass 44 AP: 0.0000\nClass 45: 37184 detections, 626 ground truths\nClass 45 AP: 0.0000\nClass 46: 113326 detections, 379 ground truths\nClass 46 AP: 0.0000\nClass 47: 234556 detections, 239 ground truths\nClass 47 AP: 0.0000\nClass 48: 116818 detections, 177 ground truths\nClass 48 AP: 0.0000\nClass 49: 20571 detections, 287 ground truths\nClass 49 AP: 0.0000\nClass 50: 153071 detections, 316 ground truths\nClass 50 AP: 0.0000\nClass 51: 27706 detections, 371 ground truths\nClass 51 AP: 0.0000\nClass 52: 46142 detections, 127 ground truths\nClass 52 AP: 0.0000\nClass 53: 19588 detections, 285 ground truths\nClass 53 AP: 0.0000\nClass 54: 7629 detections, 338 ground truths\nClass 54 AP: 0.0000\nClass 55: 33589 detections, 316 ground truths\nClass 55 AP: 0.0000\nClass 56: 81187 detections, 1791 ground truths\nClass 56 AP: 0.0000\nClass 57: 32029 detections, 261 ground truths\nClass 57 AP: 0.0000\nClass 58: 48476 detections, 343 ground truths\nClass 58 AP: 0.0000\nClass 59: 7708 detections, 163 ground truths\nClass 59 AP: 0.0000\nClass 60: 30003 detections, 697 ground truths\nClass 60 AP: 0.0000\nClass 61: 8288 detections, 179 ground truths\nClass 61 AP: 0.0000\nClass 62: 7783 detections, 288 ground truths\nClass 62 AP: 0.0000\nClass 63: 15501 detections, 231 ground truths\nClass 63 AP: 0.0000\nClass 64: 100452 detections, 106 ground truths\nClass 64 AP: 0.0000\nClass 65: 18586 detections, 283 ground truths\nClass 65 AP: 0.0000\nClass 66: 544891 detections, 153 ground truths\nClass 66 AP: 0.0000\nClass 67: 114499 detections, 262 ground truths\nClass 67 AP: 0.0000\nClass 68: 25953 detections, 55 ground truths\nClass 68 AP: 0.0000\nClass 69: 39179 detections, 143 ground truths\nClass 69 AP: 0.0000\nClass 70: 2895 detections, 9 ground truths\nClass 70 AP: 0.0000\nClass 71: 55478 detections, 225 ground truths\nClass 71 AP: 0.0000\nClass 72: 36917 detections, 126 ground truths\nClass 72 AP: 0.0000\nClass 73: 71544 detections, 1161 ground truths\nClass 73 AP: 0.0000\nClass 74: 58103 detections, 267 ground truths\nClass 74 AP: 0.0000\nClass 75: 1051728 detections, 277 ground truths\nClass 75 AP: 0.0000\nClass 76: 40097 detections, 36 ground truths\nClass 76 AP: 0.0000\nClass 77: 13663 detections, 191 ground truths\nClass 77 AP: 0.0000\nClass 78: 153439 detections, 11 ground truths\nClass 78 AP: 0.0000\nClass 79: 3421 detections, 57 ground truths\nClass 79 AP: 0.0000\nTotal valid predictions: 6404388/6404388\nmAP: 0.0000 (across 80 classes)\nEpoch 1/5:\n  Train Loss: 0.181890\n  Val Loss: 0.050121\n  Val mAP: 0.000001\n  Learning Rate: 0.000007\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1849/1849 [1:11:58<00:00,  2.34s/it, loss=0.173, lr=6.67e-6] \nValidation: 100%|██████████| 79/79 [26:12<00:00, 19.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating validation mAP...\nClass 0: 159099 detections, 11004 ground truths\nClass 0 AP: 0.0000\nClass 1: 33500 detections, 316 ground truths\nClass 1 AP: 0.0000\nClass 2: 6625 detections, 1932 ground truths\nClass 2 AP: 0.0000\nClass 3: 724 detections, 371 ground truths\nClass 3 AP: 0.0000\nClass 4: 54458 detections, 143 ground truths\nClass 4 AP: 0.0000\nClass 5: 20408 detections, 285 ground truths\nClass 5 AP: 0.0000\nClass 6: 93067 detections, 190 ground truths\nClass 6 AP: 0.0000\nClass 7: 24877 detections, 415 ground truths\nClass 7 AP: 0.0000\nClass 8: 77040 detections, 430 ground truths\nClass 8 AP: 0.0000\nClass 9: 15510 detections, 637 ground truths\nClass 9 AP: 0.0000\nClass 10: 4406 detections, 101 ground truths\nClass 10 AP: 0.0000\nClass 11: 7296 detections, 75 ground truths\nClass 11 AP: 0.0000\nClass 12: 6677 detections, 60 ground truths\nClass 12 AP: 0.0000\nClass 13: 914 detections, 413 ground truths\nClass 13 AP: 0.0000\nClass 14: 22447 detections, 440 ground truths\nClass 14 AP: 0.0000\nClass 15: 3297 detections, 202 ground truths\nClass 15 AP: 0.0000\nClass 16: 7564 detections, 218 ground truths\nClass 16 AP: 0.0000\nClass 17: 4292 detections, 273 ground truths\nClass 17 AP: 0.0000\nClass 18: 16110 detections, 361 ground truths\nClass 18 AP: 0.0000\nClass 19: 10804 detections, 380 ground truths\nClass 19 AP: 0.0000\nClass 20: 71032 detections, 255 ground truths\nClass 20 AP: 0.0000\nClass 21: 3464 detections, 71 ground truths\nClass 21 AP: 0.0000\nClass 22: 26771 detections, 268 ground truths\nClass 22 AP: 0.0000\nClass 23: 11457 detections, 232 ground truths\nClass 23 AP: 0.0000\nClass 24: 31396 detections, 371 ground truths\nClass 24 AP: 0.0000\nClass 25: 11763 detections, 413 ground truths\nClass 25 AP: 0.0000\nClass 26: 21999 detections, 540 ground truths\nClass 26 AP: 0.0000\nClass 27: 26841 detections, 254 ground truths\nClass 27 AP: 0.0000\nClass 28: 21619 detections, 303 ground truths\nClass 28 AP: 0.0000\nClass 29: 6070 detections, 115 ground truths\nClass 29 AP: 0.0000\nClass 30: 28838 detections, 241 ground truths\nClass 30 AP: 0.0000\nClass 31: 29030 detections, 69 ground truths\nClass 31 AP: 0.0000\nClass 32: 9384 detections, 263 ground truths\nClass 32 AP: 0.0000\nClass 33: 110926 detections, 336 ground truths\nClass 33 AP: 0.0000\nClass 34: 366 detections, 146 ground truths\nClass 34 AP: 0.0000\nClass 35: 1385 detections, 148 ground truths\nClass 35 AP: 0.0000\nClass 36: 6181 detections, 179 ground truths\nClass 36 AP: 0.0000\nClass 37: 50072 detections, 269 ground truths\nClass 37 AP: 0.0000\nClass 38: 30196 detections, 225 ground truths\nClass 38 AP: 0.0000\nClass 39: 7760 detections, 1025 ground truths\nClass 39 AP: 0.0000\nClass 40: 108133 detections, 343 ground truths\nClass 40 AP: 0.0000\nClass 41: 2182 detections, 899 ground truths\nClass 41 AP: 0.0000\nClass 42: 3441 detections, 215 ground truths\nClass 42 AP: 0.0000\nClass 43: 19211 detections, 326 ground truths\nClass 43 AP: 0.0000\nClass 44: 291811 detections, 253 ground truths\nClass 44 AP: 0.0000\nClass 45: 9704 detections, 626 ground truths\nClass 45 AP: 0.0000\nClass 46: 33895 detections, 379 ground truths\nClass 46 AP: 0.0000\nClass 47: 228862 detections, 239 ground truths\nClass 47 AP: 0.0000\nClass 48: 96526 detections, 177 ground truths\nClass 48 AP: 0.0000\nClass 49: 55628 detections, 287 ground truths\nClass 49 AP: 0.0000\nClass 50: 169149 detections, 316 ground truths\nClass 50 AP: 0.0000\nClass 51: 10625 detections, 371 ground truths\nClass 51 AP: 0.0000\nClass 52: 4180 detections, 127 ground truths\nClass 52 AP: 0.0000\nClass 53: 8201 detections, 285 ground truths\nClass 53 AP: 0.0000\nClass 54: 2290 detections, 338 ground truths\nClass 54 AP: 0.0000\nClass 55: 21870 detections, 316 ground truths\nClass 55 AP: 0.0000\nClass 56: 6261 detections, 1791 ground truths\nClass 56 AP: 0.0000\nClass 57: 41766 detections, 261 ground truths\nClass 57 AP: 0.0000\nClass 58: 38822 detections, 343 ground truths\nClass 58 AP: 0.0000\nClass 59: 28192 detections, 163 ground truths\nClass 59 AP: 0.0000\nClass 60: 54127 detections, 697 ground truths\nClass 60 AP: 0.0000\nClass 61: 2731 detections, 179 ground truths\nClass 61 AP: 0.0000\nClass 62: 1719 detections, 288 ground truths\nClass 62 AP: 0.0000\nClass 63: 9140 detections, 231 ground truths\nClass 63 AP: 0.0000\nClass 64: 83182 detections, 106 ground truths\nClass 64 AP: 0.0000\nClass 65: 16591 detections, 283 ground truths\nClass 65 AP: 0.0000\nClass 66: 264137 detections, 153 ground truths\nClass 66 AP: 0.0000\nClass 67: 33053 detections, 262 ground truths\nClass 67 AP: 0.0000\nClass 68: 6631 detections, 55 ground truths\nClass 68 AP: 0.0000\nClass 69: 8233 detections, 143 ground truths\nClass 69 AP: 0.0000\nClass 70: 300 detections, 9 ground truths\nClass 70 AP: 0.0000\nClass 71: 26817 detections, 225 ground truths\nClass 71 AP: 0.0000\nClass 72: 61314 detections, 126 ground truths\nClass 72 AP: 0.0000\nClass 73: 37603 detections, 1161 ground truths\nClass 73 AP: 0.0000\nClass 74: 10967 detections, 267 ground truths\nClass 74 AP: 0.0000\nClass 75: 551261 detections, 277 ground truths\nClass 75 AP: 0.0000\nClass 76: 40466 detections, 36 ground truths\nClass 76 AP: 0.0000\nClass 77: 3941 detections, 191 ground truths\nClass 77 AP: 0.0000\nClass 78: 36860 detections, 11 ground truths\nClass 78 AP: 0.0000\nClass 79: 1668 detections, 57 ground truths\nClass 79 AP: 0.0000\nTotal valid predictions: 3507155/3507155\nmAP: 0.0000 (across 80 classes)\nEpoch 2/5:\n  Train Loss: 0.053150\n  Val Loss: 0.030731\n  Val mAP: 0.000000\n  Learning Rate: 0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1849/1849 [1:11:35<00:00,  2.32s/it, loss=0.136, lr=1e-5] \nValidation: 100%|██████████| 79/79 [13:01<00:00,  9.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating validation mAP...\nClass 0: 149964 detections, 11004 ground truths\nClass 0 AP: 0.0000\nClass 1: 5973 detections, 316 ground truths\nClass 1 AP: 0.0000\nClass 2: 26029 detections, 1932 ground truths\nClass 2 AP: 0.0000\nClass 3: 1220 detections, 371 ground truths\nClass 3 AP: 0.0000\nClass 4: 7205 detections, 143 ground truths\nClass 4 AP: 0.0000\nClass 5: 6825 detections, 285 ground truths\nClass 5 AP: 0.0000\nClass 6: 40286 detections, 190 ground truths\nClass 6 AP: 0.0000\nClass 7: 6198 detections, 415 ground truths\nClass 7 AP: 0.0000\nClass 8: 11502 detections, 430 ground truths\nClass 8 AP: 0.0000\nClass 9: 9463 detections, 637 ground truths\nClass 9 AP: 0.0000\nClass 10: 534 detections, 101 ground truths\nClass 10 AP: 0.0000\nClass 11: 1824 detections, 75 ground truths\nClass 11 AP: 0.0000\nClass 12: 628 detections, 60 ground truths\nClass 12 AP: 0.0000\nClass 13: 264 detections, 413 ground truths\nClass 13 AP: 0.0000\nClass 14: 30309 detections, 440 ground truths\nClass 14 AP: 0.0000\nClass 15: 2817 detections, 202 ground truths\nClass 15 AP: 0.0000\nClass 16: 1802 detections, 218 ground truths\nClass 16 AP: 0.0000\nClass 17: 361 detections, 273 ground truths\nClass 17 AP: 0.0000\nClass 18: 14572 detections, 361 ground truths\nClass 18 AP: 0.0000\nClass 19: 1295 detections, 380 ground truths\nClass 19 AP: 0.0000\nClass 20: 13501 detections, 255 ground truths\nClass 20 AP: 0.0000\nClass 21: 323 detections, 71 ground truths\nClass 21 AP: 0.0000\nClass 22: 13269 detections, 268 ground truths\nClass 22 AP: 0.0000\nClass 23: 5283 detections, 232 ground truths\nClass 23 AP: 0.0000\nClass 24: 25970 detections, 371 ground truths\nClass 24 AP: 0.0000\nClass 25: 4835 detections, 413 ground truths\nClass 25 AP: 0.0000\nClass 26: 6125 detections, 540 ground truths\nClass 26 AP: 0.0000\nClass 27: 196 detections, 254 ground truths\nClass 27 AP: 0.0000\nClass 28: 2170 detections, 303 ground truths\nClass 28 AP: 0.0000\nClass 29: 583 detections, 115 ground truths\nClass 29 AP: 0.0000\nClass 30: 5165 detections, 241 ground truths\nClass 30 AP: 0.0000\nClass 31: 10439 detections, 69 ground truths\nClass 31 AP: 0.0000\nClass 32: 2636 detections, 263 ground truths\nClass 32 AP: 0.0000\nClass 33: 55867 detections, 336 ground truths\nClass 33 AP: 0.0000\nClass 34: 500 detections, 146 ground truths\nClass 34 AP: 0.0000\nClass 35: 249 detections, 148 ground truths\nClass 35 AP: 0.0000\nClass 36: 214 detections, 179 ground truths\nClass 36 AP: 0.0000\nClass 37: 24715 detections, 269 ground truths\nClass 37 AP: 0.0000\nClass 38: 38 detections, 225 ground truths\nClass 38 AP: 0.0000\nClass 39: 15355 detections, 1025 ground truths\nClass 39 AP: 0.0000\nClass 40: 59138 detections, 343 ground truths\nClass 40 AP: 0.0000\nClass 41: 1412 detections, 899 ground truths\nClass 41 AP: 0.0000\nClass 42: 1275 detections, 215 ground truths\nClass 42 AP: 0.0000\nClass 43: 3954 detections, 326 ground truths\nClass 43 AP: 0.0000\nClass 44: 76066 detections, 253 ground truths\nClass 44 AP: 0.0000\nClass 45: 6648 detections, 626 ground truths\nClass 45 AP: 0.0000\nClass 46: 2866 detections, 379 ground truths\nClass 46 AP: 0.0000\nClass 47: 152422 detections, 239 ground truths\nClass 47 AP: 0.0000\nClass 48: 60835 detections, 177 ground truths\nClass 48 AP: 0.0000\nClass 49: 10614 detections, 287 ground truths\nClass 49 AP: 0.0000\nClass 50: 60067 detections, 316 ground truths\nClass 50 AP: 0.0000\nClass 51: 2615 detections, 371 ground truths\nClass 51 AP: 0.0000\nClass 52: 476 detections, 127 ground truths\nClass 52 AP: 0.0000\nClass 53: 1489 detections, 285 ground truths\nClass 53 AP: 0.0000\nClass 54: 793 detections, 338 ground truths\nClass 54 AP: 0.0000\nClass 55: 15812 detections, 316 ground truths\nClass 55 AP: 0.0000\nClass 56: 5600 detections, 1791 ground truths\nClass 56 AP: 0.0000\nClass 57: 29991 detections, 261 ground truths\nClass 57 AP: 0.0000\nClass 58: 23167 detections, 343 ground truths\nClass 58 AP: 0.0000\nClass 59: 3419 detections, 163 ground truths\nClass 59 AP: 0.0000\nClass 60: 45330 detections, 697 ground truths\nClass 60 AP: 0.0000\nClass 61: 513 detections, 179 ground truths\nClass 61 AP: 0.0000\nClass 62: 374 detections, 288 ground truths\nClass 62 AP: 0.0000\nClass 63: 8655 detections, 231 ground truths\nClass 63 AP: 0.0000\nClass 64: 9658 detections, 106 ground truths\nClass 64 AP: 0.0000\nClass 65: 1115 detections, 283 ground truths\nClass 65 AP: 0.0000\nClass 66: 102384 detections, 153 ground truths\nClass 66 AP: 0.0000\nClass 67: 6699 detections, 262 ground truths\nClass 67 AP: 0.0000\nClass 68: 761 detections, 55 ground truths\nClass 68 AP: 0.0000\nClass 69: 3366 detections, 143 ground truths\nClass 69 AP: 0.0000\nClass 70: 16 detections, 9 ground truths\nClass 70 AP: 0.0000\nClass 71: 8240 detections, 225 ground truths\nClass 71 AP: 0.0000\nClass 72: 48961 detections, 126 ground truths\nClass 72 AP: 0.0000\nClass 73: 32138 detections, 1161 ground truths\nClass 73 AP: 0.0000\nClass 74: 5346 detections, 267 ground truths\nClass 74 AP: 0.0000\nClass 75: 198520 detections, 277 ground truths\nClass 75 AP: 0.0000\nClass 76: 25039 detections, 36 ground truths\nClass 76 AP: 0.0000\nClass 77: 724 detections, 191 ground truths\nClass 77 AP: 0.0000\nClass 78: 14469 detections, 11 ground truths\nClass 78 AP: 0.0000\nClass 79: 474 detections, 57 ground truths\nClass 79 AP: 0.0000\nTotal valid predictions: 1537945/1537945\nmAP: 0.0000 (across 80 classes)\nEpoch 3/5:\n  Train Loss: 0.031892\n  Val Loss: 0.025604\n  Val mAP: 0.000000\n  Learning Rate: 0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  12%|█▏        | 213/1849 [08:19<1:03:58,  2.35s/it, loss=0.028, lr=1e-5] ","output_type":"stream"}],"execution_count":null}]}